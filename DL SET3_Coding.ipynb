{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6476829-0502-4486-8149-b84a7cb4ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n",
      "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5\n",
      "\u001b[1m85521592/85521592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m model = sm.Unet(\u001b[33m'\u001b[39m\u001b[33mresnet34\u001b[39m\u001b[33m'\u001b[39m, classes=\u001b[32m1\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33msigmoid\u001b[39m\u001b[33m'\u001b[39m, encoder_weights=\u001b[33m'\u001b[39m\u001b[33mimagenet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 2. Prepare Image (Resize to a multiple of 32 for U-Net)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m img_raw = load_img(\u001b[33m\"\u001b[39m\u001b[33mimage.jpg\u001b[39m\u001b[33m\"\u001b[39m, target_size=(\u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m))\n\u001b[32m     16\u001b[39m img_array = img_to_array(img_raw) / \u001b[32m255.0\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 3. Predict & Display\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/keras/src/utils/image_utils.py:247\u001b[39m, in \u001b[36mload_img\u001b[39m\u001b[34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[39m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib.Path):\n\u001b[32m    246\u001b[39m         path = \u001b[38;5;28mstr\u001b[39m(path.resolve())\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    248\u001b[39m         img = pil_image.open(io.BytesIO(f.read()))\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'image.jpg'"
     ]
    }
   ],
   "source": [
    "#Load a pretrained UNet model and perform pixel-wise segmentation on new images.\n",
    "#!pip install -U segmentation-models\n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\" # Force library to use tensorflow.keras\n",
    "\n",
    "import segmentation_models as sm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# 1. Load Model (e.g., using ResNet backbone)\n",
    "model = sm.Unet('resnet34', classes=1, activation='sigmoid', encoder_weights='imagenet')\n",
    "\n",
    "# 2. Prepare Image (Resize to a multiple of 32 for U-Net)\n",
    "img_raw = load_img(\"image.jpg\", target_size=(256, 256))\n",
    "img_array = img_to_array(img_raw) / 255.0\n",
    "\n",
    "# 3. Predict & Display\n",
    "mask = model.predict(np.expand_dims(img_array, axis=0))[0]\n",
    "plt.imshow(img_raw)\n",
    "plt.imshow(mask > 0.5, alpha=0.5) # Overlay mask\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "953c446c-ba0d-427d-a591-7bd477a030f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 15ms/step - accuracy: 0.9570 - loss: 0.1436 - val_accuracy: 0.9849 - val_loss: 0.0471\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.9850 - loss: 0.0481 - val_accuracy: 0.9851 - val_loss: 0.0461\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 20ms/step - accuracy: 0.9888 - loss: 0.0342 - val_accuracy: 0.9896 - val_loss: 0.0317\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9922 - loss: 0.0248 - val_accuracy: 0.9891 - val_loss: 0.0341\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9940 - loss: 0.0192 - val_accuracy: 0.9921 - val_loss: 0.0246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14a004990>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build a CNN that classifies handwritten digits using the MNIST dataset.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixels to [0, 1] and reshape to (28, 28, 1)\n",
    "x_train = x_train.reshape((-1, 28, 28, 1)).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape((-1, 28, 28, 1)).astype(\"float32\") / 255.0\n",
    "model = models.Sequential([\n",
    "    # First Convolutional block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second Convolutional block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten and Classify\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax') # 10 digits (0-9)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train for 5 epochs\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59ee03a4-b042-4e4f-9014-c3733190e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function that reads an image, resizes it, and normalizes it for inference.\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    # 1. Read image (OpenCV reads in BGR by default)\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found at the specified path\")\n",
    "    \n",
    "    # 2. Convert to RGB and Resize\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # 3. Normalize to [0, 1] and add batch dimension\n",
    "    img_array = img.astype('float32') / 255.0\n",
    "    img_batch = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89bc7a23-5df8-4cac-a357-b09dda9ccba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.249-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (3.10.7)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (12.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (1.16.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (0.17.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from ultralytics) (5.9.0)\n",
      "Collecting polars>=0.20.0 (from ultralytics)\n",
      "  Downloading polars-1.36.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
      "  Using cached ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Collecting polars-runtime-32==1.36.1 (from polars>=0.20.0->ultralytics)\n",
      "  Downloading polars_runtime_32-1.36.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2025.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.3.249-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "Downloading polars-1.36.1-py3-none-any.whl (802 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading polars_runtime_32-1.36.1-cp39-abi3-macosx_10_12_x86_64.whl (43.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: polars-runtime-32, polars, ultralytics-thop, ultralytics\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [ultralytics]\u001b[0m [ultralytics]thop]\n",
      "Successfully installed polars-1.36.1 polars-runtime-32-1.36.1 ultralytics-8.3.249 ultralytics-thop-2.0.18\n",
      "8.3.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/harikrishnan/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(ultralytics.__version__)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = torch.hub.load(\u001b[33m'\u001b[39m\u001b[33multralytics/yolov5\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33myolov5s\u001b[39m\u001b[33m'\u001b[39m, pretrained=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Path to your image (or a URL)\u001b[39;00m\n\u001b[32m     12\u001b[39m img = \u001b[33m'\u001b[39m\u001b[33mpath/to/vehicle_image.jpg\u001b[39m\u001b[33m'\u001b[39m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/hub.py:566\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source == \u001b[33m'\u001b[39m\u001b[33mgithub\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    563\u001b[39m     repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[33m\"\u001b[39m\u001b[33mload\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    564\u001b[39m                                        verbose=verbose, skip_validation=skip_validation)\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m model = _load_local(repo_or_dir, model, *args, **kwargs)\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/hub.py:595\u001b[39m, in \u001b[36m_load_local\u001b[39m\u001b[34m(hubconf_dir, model, *args, **kwargs)\u001b[39m\n\u001b[32m    592\u001b[39m     hub_module = _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[32m    594\u001b[39m     entry = _load_entry_from_hubconf(hub_module, model)\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m     model = entry(*args, **kwargs)\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:213\u001b[39m, in \u001b[36myolov5s\u001b[39m\u001b[34m(pretrained, channels, classes, autoshape, _verbose, device)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34myolov5s\u001b[39m(pretrained=\u001b[38;5;28;01mTrue\u001b[39;00m, channels=\u001b[32m3\u001b[39m, classes=\u001b[32m80\u001b[39m, autoshape=\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose=\u001b[38;5;28;01mTrue\u001b[39;00m, device=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    176\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a YOLOv5-small (yolov5s) model with options for pretraining, input channels, class count, autoshaping,\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[33;03m    verbosity, and device configuration.\u001b[39;00m\n\u001b[32m    178\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    211\u001b[39m \u001b[33;03m        the [YOLOv5 PyTorch Hub Documentation](https://pytorch.org/hub/ultralytics_yolov5/).\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _create(\u001b[33m\"\u001b[39m\u001b[33myolov5s\u001b[39m\u001b[33m\"\u001b[39m, pretrained, channels, classes, autoshape, _verbose, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:55\u001b[39m, in \u001b[36m_create\u001b[39m\u001b[34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates or loads a YOLOv5 model, with options for pretrained weights and model customization.\u001b[39;00m\n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m    [YOLOv5 PyTorch Hub Documentation](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/).\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoShape, DetectMultiBackend\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m attempt_load\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01myolo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ClassificationModel, DetectionModel, SegmentationModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:39\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplotting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Annotator, colors, save_one_box\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TryExcept\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exif_transpose, letterbox\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     41\u001b[39m     LOGGER,\n\u001b[32m     42\u001b[39m     ROOT,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m     yaml_load,\n\u001b[32m     56\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m copy_attr, smart_inference_mode\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/utils/dataloaders.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExifTags, Image, ImageOps\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset, dataloader, distributed\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     Albumentations,\n\u001b[32m     31\u001b[39m     augment_hsv,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     random_perspective,\n\u001b[32m     38\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     DATASETS_DIR,\n\u001b[32m     41\u001b[39m     LOGGER,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     xyxy2xywhn,\n\u001b[32m     57\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "#detect objects YOLO\n",
    "import torch\n",
    "\n",
    "# Load the pretrained YOLOv5s model from PyTorch Hub\n",
    "!pip install -U ultralytics\n",
    "\n",
    "import ultralytics\n",
    "print(ultralytics.__version__)\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "# Path to your image (or a URL)\n",
    "img = 'path/to/vehicle_image.jpg' \n",
    "\n",
    "# Run the model\n",
    "results = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e33139-7032-48df-9b86-ae54d8a544a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Purchase Probability: 0.42\n"
     ]
    }
   ],
   "source": [
    "#Use an Artificial Neural Network to predict whether a customer will buy a product (0 = No, 1 = Yes), based on Age and Salary.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data: [Age, Salary]\n",
    "X = np.array([[25, 30000], [35, 70000], [45, 120000], [20, 15000], [55, 60000]])\n",
    "y = np.array([0, 1, 1, 0, 1])\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Input Layer + First Hidden Layer\n",
    "    tf.keras.layers.Dense(units=6, activation='relu', input_dim=2),\n",
    "    # Second Hidden Layer\n",
    "    tf.keras.layers.Dense(units=6, activation='relu'),\n",
    "    # Output Layer (Sigmoid for binary probability)\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_scaled, y, epochs=100, verbose=0)\n",
    "\n",
    "# Predict for a new customer (Age: 30, Salary: 50,000)\n",
    "new_data = scaler.transform([[30, 50000]])\n",
    "prediction = model.predict(new_data)\n",
    "print(f\"Purchase Probability: {prediction[0][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded365a8-771c-4037-a89f-ffad4a72fbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
