{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3308faa-0708-424e-820d-8d1d8545df72",
   "metadata": {},
   "source": [
    "# Batch1-Coding Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b92be6-ea8b-42fe-9519-e1ccebbbe149",
   "metadata": {},
   "source": [
    "# Question 1: Image Preprocessing for Inference (PyTorch)\n",
    "# Write a function to load an image and preprocess it for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca76713d-e1ba-4c1b-9995-e253ad3c73f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.2.2 (from torchvision)\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torchvision) (12.0.0)\n",
      "Collecting filelock (from torch==2.2.2->torchvision)\n",
      "  Using cached filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (4.12.2)\n",
      "Collecting sympy (from torch==2.2.2->torchvision)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.2.2->torchvision)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.1.6)\n",
      "Collecting fsspec (from torch==2.2.2->torchvision)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages (from jinja2->torch==2.2.2->torchvision) (3.0.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.2->torchvision)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.20.2-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchvision\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [torchvision]\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.2 fsspec-2025.12.0 mpmath-1.3.0 networkx-3.6.1 sympy-1.14.0 torch-2.2.2 torchvision-0.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5e91b9-5b55-4d32-afe3-c7a03bf0a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1f1d8-0dd7-441e-a4a5-63a8d6230125",
   "metadata": {},
   "source": [
    "# Question 2: Predict on New Image with a Trained Model\n",
    "# Perform prediction and get the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758a9ad2-734f-4339-acc1-7d7122006a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/harikrishnan/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m model = models.resnet18(pretrained=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m model.eval()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m input_image = preprocess_image(\u001b[33m\"\u001b[39m\u001b[33mtest.jpg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     11\u001b[39m     output = model(input_image)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpreprocess_image\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_image\u001b[39m(image_path):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     image = Image.open(image_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     transform = transforms.Compose([\n\u001b[32m      7\u001b[39m         transforms.Resize((\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)),\n\u001b[32m      8\u001b[39m         transforms.ToTensor(),\n\u001b[32m      9\u001b[39m         transforms.Normalize([\u001b[32m0.485\u001b[39m,\u001b[32m0.456\u001b[39m,\u001b[32m0.406\u001b[39m], [\u001b[32m0.229\u001b[39m,\u001b[32m0.224\u001b[39m,\u001b[32m0.225\u001b[39m])\n\u001b[32m     10\u001b[39m     ])\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(image).unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/PIL/Image.py:3493\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3492\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3493\u001b[39m     fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3494\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'test.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "input_image = preprocess_image(\"test.jpg\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "    predicted_class = output.argmax(1).item()\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1728a-405d-421f-9d21-9a4d5620b0f8",
   "metadata": {},
   "source": [
    "# Question 3: Build a CNN to classify CIFAR-10 images (PyTorch)\n",
    "# Create a CNN model that classifies images from the CIFAR-10 dataset with accuracy above 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb9b03d-a48a-462c-9b26-b5f8a13cac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*8*8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training (1 epoch shown for simplicity)\n",
    "for epoch in range(1):\n",
    "    for images, labels in trainloader:\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6935eb8-e2a9-492d-95f0-e1dc58506aef",
   "metadata": {},
   "source": [
    "# Question 4: Identify Overfitting from Training Logs and Solve It\n",
    "# Problem: You notice the training accuracy increases but validation accuracy stagnates. Modify the model using dropout and early stopping(use mnist dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "013896dd-4f01-4140-a1ee-21478c7c193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 11:05:51.716804: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "/Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9118 - loss: 0.2922 - val_accuracy: 0.9645 - val_loss: 0.1138\n",
      "Epoch 2/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9570 - loss: 0.1435 - val_accuracy: 0.9715 - val_loss: 0.0916\n",
      "Epoch 3/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9648 - loss: 0.1168 - val_accuracy: 0.9758 - val_loss: 0.0825\n",
      "Epoch 4/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9699 - loss: 0.0980 - val_accuracy: 0.9769 - val_loss: 0.0744\n",
      "Epoch 5/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9737 - loss: 0.0850 - val_accuracy: 0.9764 - val_loss: 0.0752\n",
      "Epoch 6/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9767 - loss: 0.0763 - val_accuracy: 0.9784 - val_loss: 0.0717\n",
      "Epoch 7/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9772 - loss: 0.0719 - val_accuracy: 0.9814 - val_loss: 0.0669\n",
      "Epoch 8/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9784 - loss: 0.0669 - val_accuracy: 0.9813 - val_loss: 0.0692\n",
      "Epoch 9/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0601 - val_accuracy: 0.9802 - val_loss: 0.0696\n",
      "Epoch 10/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9809 - loss: 0.0588 - val_accuracy: 0.9813 - val_loss: 0.0662\n",
      "Epoch 11/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9824 - loss: 0.0565 - val_accuracy: 0.9810 - val_loss: 0.0719\n",
      "Epoch 12/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9839 - loss: 0.0512 - val_accuracy: 0.9821 - val_loss: 0.0730\n",
      "Epoch 13/30\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.0510 - val_accuracy: 0.9824 - val_loss: 0.0704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x123511d50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=30, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9947878-50d4-44ae-a254-670898589add",
   "metadata": {},
   "source": [
    "# Question 5: Transfer Learning with Pretrained VGG16 (Cats vs Dogs)\n",
    "# Problem: Use VGG16 for binary classification with fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189ad90e-8648-4630-992b-d5fad8a5d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "base_model = VGG16(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379d4d9-f3c8-4d5f-bce0-70beb6b1aef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
