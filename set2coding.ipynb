{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a27aacc-7f7a-465f-999d-a2ed04696f9a",
   "metadata": {},
   "source": [
    "## Batch 2 - Coding Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131b0c4-a4ec-4055-b8de-fa386fcd806e",
   "metadata": {},
   "source": [
    "# 1. Object Detection with YOLOv5 (Using Pretrained Weights)\n",
    "# Problem: Detect objects using pre-trained YOLOv5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d462d564-b9f2-4594-9472-969ef02ecf15",
   "metadata": {},
   "source": [
    "# !pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load model\n",
    "model = YOLO('yolov5s.pt')  # Small YOLOv5 model\n",
    "\n",
    "# Inference\n",
    "results = model('sample.jpg')  # Image path\n",
    "\n",
    "# Show predictions\n",
    "results.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc11cf-c336-4771-9399-9a5b64cecbb5",
   "metadata": {},
   "source": [
    "# 2.Preprocessing Image for ResNet Input\n",
    "# Problem: Write a function to preprocess image for pretrained ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b97fd4-c782-4ff7-9dad-2fb670da67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ceb879-fd03-4d99-9ec6-4ecf01eda8de",
   "metadata": {},
   "source": [
    "# 3. CNN for CIFAR-10 Classification\n",
    "# Problem: Build a CNN model to classify CIFAR-10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd5dbbc-e01b-483f-aed8-c0b406f878fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:42:49.125544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f5d08-7399-408e-aab0-79a8f7174088",
   "metadata": {},
   "source": [
    "# 4. Scenario: Face Mask Detection – Real-Time Webcam Classifier\n",
    "# Problem: Build a real-time face mask detector using a trained CNN model and OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e679ce78-eddc-4fe5-ab03-62d9f7f1623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e1e138-47ed-40fe-9d43-f313a89c84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy training data (100 images, 32x32 RGB)\n",
    "x_train = np.random.rand(100, 32, 32, 3)\n",
    "y_train = np.random.randint(0, 2, 100)\n",
    "\n",
    "x_test = np.random.rand(20, 32, 32, 3)\n",
    "y_test = np.random.randint(0, 2, 20)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22c2718-afd9-4101-a2fe-38163346c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harikrishnan/Documents/anaconda3/envs/myenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996bde84-3dcd-4c67-a4ad-fa629b2d61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f27a71e-67ef-41ea-9f58-add0bad90e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.5300 - loss: 0.7382 - val_accuracy: 0.3000 - val_loss: 0.9042\n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5500 - loss: 0.7607 - val_accuracy: 0.3000 - val_loss: 0.7106\n",
      "Epoch 3/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5900 - loss: 0.6843 - val_accuracy: 0.7000 - val_loss: 0.6744\n",
      "Epoch 4/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5400 - loss: 0.6905 - val_accuracy: 0.3000 - val_loss: 0.7090\n",
      "Epoch 5/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7011 - val_accuracy: 0.3000 - val_loss: 0.7482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x169cb3c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec6069c-439a-437d-a896-9df2d3d79ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "model.save(\"mask_detector.h5\")\n",
    "print(\"Model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "197039cf-26c2-4214-a72c-740129ce0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(\"mask_detector.h5\")\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92a020ab-fefe-484a-b180-127c4dbb4bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "sample_image = np.random.rand(1, 32, 32, 3)\n",
    "prediction = loaded_model.predict(sample_image)\n",
    "\n",
    "predicted_class = np.argmax(prediction)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e57f6-97da-4c9a-be25-74a48d1e08a5",
   "metadata": {},
   "source": [
    "# 5. Scenario: Custom Transfer Learning with Frozen + Trainable Layers\n",
    "# Problem: Load a pretrained MobileNetV2 model, freeze base layers, add custom classifier, and fine-tune the top few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef02bb2-dc3f-45d5-ab6d-98e2df278ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e47406-50de-41a3-bca3-ab66998f8758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
